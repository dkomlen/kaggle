{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Class']\n",
    "X = data.drop(['Time', 'Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017272075982947815"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[y_train == 1]) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001755001755001755"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[y_test == 1]) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "lg = LogisticRegression(random_state=42, verbose=1, max_iter=10000)\n",
    "MLPC = MLPClassifier(hidden_layer_sizes=(200,), max_iter=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=42, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 0.00733786\n",
      "Iteration 3, loss = 0.01078998\n",
      "Iteration 4, loss = 0.00634355\n",
      "Iteration 5, loss = 0.01097162\n",
      "Iteration 6, loss = 0.00537894\n",
      "Iteration 7, loss = 0.00848033\n",
      "Iteration 8, loss = 0.00459356\n",
      "Iteration 9, loss = 0.00839671\n",
      "Iteration 10, loss = inf\n",
      "Iteration 11, loss = 0.00488251\n",
      "Iteration 12, loss = 0.00599491\n",
      "Iteration 13, loss = 0.00549640\n",
      "Iteration 14, loss = 0.00977681\n",
      "Iteration 15, loss = 0.00409438\n",
      "Iteration 16, loss = inf\n",
      "Iteration 17, loss = 0.00733932\n",
      "Iteration 18, loss = 0.00607987\n",
      "Iteration 19, loss = 0.00370983\n",
      "Iteration 20, loss = 0.01025396\n",
      "Iteration 21, loss = inf\n",
      "Iteration 22, loss = inf\n",
      "Iteration 23, loss = 0.00454203\n",
      "Iteration 24, loss = 0.00389041\n",
      "Iteration 25, loss = 0.00432095\n",
      "Iteration 26, loss = 0.00354256\n",
      "Iteration 27, loss = 0.00425857\n",
      "Iteration 28, loss = 0.00470491\n",
      "Iteration 29, loss = 0.00356677\n",
      "Iteration 30, loss = 0.00334311\n",
      "Iteration 31, loss = 0.00597962\n",
      "Iteration 32, loss = inf\n",
      "Iteration 33, loss = 0.00396576\n",
      "Iteration 34, loss = 0.00679228\n",
      "Iteration 35, loss = 0.00326969\n",
      "Iteration 36, loss = 0.00354408\n",
      "Iteration 37, loss = 0.00304008\n",
      "Iteration 38, loss = 0.00626858\n",
      "Iteration 39, loss = 0.00374813\n",
      "Iteration 40, loss = 0.00339150\n",
      "Iteration 41, loss = 0.00354112\n",
      "Iteration 42, loss = 0.00306270\n",
      "Iteration 43, loss = 0.00262636\n",
      "Iteration 44, loss = 0.00333631\n",
      "Iteration 45, loss = 0.00293897\n",
      "Iteration 46, loss = 0.00262138\n",
      "Iteration 47, loss = 0.00287486\n",
      "Iteration 48, loss = 0.00275042\n",
      "Iteration 49, loss = 0.00258669\n",
      "Iteration 50, loss = 0.00227048\n",
      "Iteration 51, loss = 0.00215379\n",
      "Iteration 52, loss = 0.00350823\n",
      "Iteration 53, loss = 0.00249054\n",
      "Iteration 54, loss = 0.00219490\n",
      "Iteration 55, loss = 0.00298425\n",
      "Iteration 56, loss = 0.00260004\n",
      "Iteration 57, loss = 0.00250609\n",
      "Iteration 58, loss = inf\n",
      "Iteration 59, loss = 0.00248208\n",
      "Iteration 60, loss = 0.00270410\n",
      "Iteration 61, loss = 0.00190772\n",
      "Iteration 62, loss = 0.00204330\n",
      "Iteration 63, loss = 0.00424942\n",
      "Iteration 64, loss = 0.00212263\n",
      "Iteration 65, loss = 0.00184324\n",
      "Iteration 66, loss = 0.00204439\n",
      "Iteration 67, loss = 0.00266449\n",
      "Iteration 68, loss = 0.00278112\n",
      "Iteration 69, loss = 0.00208975\n",
      "Iteration 70, loss = 0.00167119\n",
      "Iteration 71, loss = 0.00203869\n",
      "Iteration 72, loss = 0.00202662\n",
      "Iteration 73, loss = 0.00180396\n",
      "Iteration 74, loss = 0.00202020\n",
      "Iteration 75, loss = inf\n",
      "Iteration 76, loss = 0.00166466\n",
      "Iteration 77, loss = 0.00180818\n",
      "Iteration 78, loss = 0.00147182\n",
      "Iteration 79, loss = 0.00196009\n",
      "Iteration 80, loss = 0.00202947\n",
      "Iteration 81, loss = 0.00208779\n",
      "Iteration 82, loss = 0.00208268\n",
      "Iteration 83, loss = 0.00246617\n",
      "Iteration 84, loss = 0.00259663\n",
      "Iteration 85, loss = 0.00188569\n",
      "Iteration 86, loss = 0.00222828\n",
      "Iteration 87, loss = 0.00241157\n",
      "Iteration 88, loss = 0.00183260\n",
      "Iteration 89, loss = 0.00245293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(200,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037037037037037"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = MLPC.predict(X_test)\n",
    "recall_score (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6234567901234568"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lg.predict(X_test)\n",
    "recall_score (y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_idx = np.array(data[data.Class==1].index)\n",
    "normal_idx = np.array(data[data.Class==0].index)\n",
    "normal_sample_idx = np.random.choice(normal_idx, len(fraud_idx), replace=False)\n",
    "\n",
    "X_sample = data.iloc[np.concatenate([normal_sample_idx, fraud_idx])]\n",
    "y_sample = X_sample['Class']\n",
    "X_sample = X_sample.drop(['Time', 'Class'], axis=1)\n",
    "\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X_sample, y_sample, test_size=0.33, random_state=42, stratify=y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = inf\n",
      "Iteration 3, loss = 1.24759912\n",
      "Iteration 4, loss = 0.71865233\n",
      "Iteration 5, loss = 0.64946483\n",
      "Iteration 6, loss = 0.60455492\n",
      "Iteration 7, loss = 0.40459439\n",
      "Iteration 8, loss = 0.40082153\n",
      "Iteration 9, loss = 0.35644634\n",
      "Iteration 10, loss = 0.30450621\n",
      "Iteration 11, loss = 0.29855871\n",
      "Iteration 12, loss = 0.27003296\n",
      "Iteration 13, loss = 0.26676924\n",
      "Iteration 14, loss = 0.27920768\n",
      "Iteration 15, loss = 0.23859472\n",
      "Iteration 16, loss = 0.28874383\n",
      "Iteration 17, loss = 0.22291210\n",
      "Iteration 18, loss = 0.20452491\n",
      "Iteration 19, loss = 0.20712640\n",
      "Iteration 20, loss = 0.18964946\n",
      "Iteration 21, loss = 0.18575479\n",
      "Iteration 22, loss = 0.18257243\n",
      "Iteration 23, loss = 0.17197556\n",
      "Iteration 24, loss = 0.16800077\n",
      "Iteration 25, loss = 0.16113604\n",
      "Iteration 26, loss = 0.16852578\n",
      "Iteration 27, loss = 0.20878871\n",
      "Iteration 28, loss = 0.18444433\n",
      "Iteration 29, loss = 0.16683042\n",
      "Iteration 30, loss = 0.18340362\n",
      "Iteration 31, loss = 0.16715476\n",
      "Iteration 32, loss = 0.14350767\n",
      "Iteration 33, loss = 0.16305255\n",
      "Iteration 34, loss = 0.14851096\n",
      "Iteration 35, loss = 0.15576796\n",
      "Iteration 36, loss = 0.13119854\n",
      "Iteration 37, loss = 0.17249306\n",
      "Iteration 38, loss = 0.14579155\n",
      "Iteration 39, loss = 0.15701486\n",
      "Iteration 40, loss = 0.13496219\n",
      "Iteration 41, loss = 0.13511193\n",
      "Iteration 42, loss = 0.14244040\n",
      "Iteration 43, loss = 0.12159991\n",
      "Iteration 44, loss = 0.12569687\n",
      "Iteration 45, loss = 0.13005613\n",
      "Iteration 46, loss = 0.13891351\n",
      "Iteration 47, loss = 0.11081952\n",
      "Iteration 48, loss = 0.11405641\n",
      "Iteration 49, loss = 0.10987440\n",
      "Iteration 50, loss = 0.10887871\n",
      "Iteration 51, loss = 0.11174026\n",
      "Iteration 52, loss = 0.10521296\n",
      "Iteration 53, loss = 0.11828631\n",
      "Iteration 54, loss = 0.11816093\n",
      "Iteration 55, loss = 0.14730683\n",
      "Iteration 56, loss = 0.10088529\n",
      "Iteration 57, loss = 0.12727443\n",
      "Iteration 58, loss = 0.15240217\n",
      "Iteration 59, loss = 0.15148156\n",
      "Iteration 60, loss = 0.13978179\n",
      "Iteration 61, loss = 0.15628322\n",
      "Iteration 62, loss = 0.14529818\n",
      "Iteration 63, loss = 0.11742536\n",
      "Iteration 64, loss = 0.10949053\n",
      "Iteration 65, loss = 0.13140765\n",
      "Iteration 66, loss = 0.10441851\n",
      "Iteration 67, loss = 0.11631955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(200,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPC.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8950617283950617"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = MLPC.predict(X_test)\n",
    "recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
